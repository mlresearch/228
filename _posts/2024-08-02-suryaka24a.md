---
title: Pitfalls in Measuring Neural Transferability
abstract: Transferability scores quantify the aptness of the pre-trained models for
  a downstream task and help in selecting an optimal pre-trained model for transfer
  learning. This work aims to draw attention to the significant shortcomings of state-of-the-art
  transferability scores. To this aim, we propose \emph{neural collapse-based transferability
  scores} that analyse intra-class \emph{variability collapse} and inter-class discriminative
  ability of the penultimate embedding space of a pre-trained model. The experimentation
  across the image and audio domains demonstrates that such a simple variability analysis
  of the feature space is sufficient to satisfy the current definition of transferability
  scores, and there is a requirement for a new generic definition of transferability.
  Further, building on these results, we highlight new research directions and postulate
  characteristics of an ideal transferability measure that will be helpful in streamlining
  future studies targeting this problem.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: suryaka24a
month: 0
tex_title: Pitfalls in Measuring Neural Transferability
firstpage: 279
lastpage: 291
page: 279-291
order: 279
cycles: false
bibtex_author: Suryaka, Suresh and Vinayak, Abrol and Anshul, Thakur
author:
- given: Suresh
  family: Suryaka
- given: Abrol
  family: Vinayak
- given: Thakur
  family: Anshul
date: 2024-08-02
address:
container-title: Proceedings of the 2nd NeurIPS Workshop on Symmetry and Geometry
  in Neural Representations
volume: '228'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 8
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v228/main/assets/suryaka24a/suryaka24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
