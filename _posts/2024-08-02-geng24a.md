---
title: Scalar Invariant Networks with Zero Bias
abstract: Just like weights, bias terms are learnable parameters in many popular machine
  learning models, including neural networks. Biases are believed to enhance the representational
  power of neural networks, enabling them to tackle various tasks in computer vision.
  Nevertheless, we argue that biases can be disregarded for some image-related tasks
  such as image classification, by considering the intrinsic distribution of images
  in the input space and desired model properties from first principles. Our empirical
  results suggest that zero-bias neural networks can perform comparably to normal
  networks for practical image classification tasks. Furthermore, we demonstrate that
  zero-bias neural networks possess a valuable property known as scalar (multiplicative)
  invariance. This implies that the networkâ€™s predictions remain unchanged even when
  the contrast of the input image is altered. We further extend the scalar invariance
  property to more general cases, thereby attaining robustness within specific convex
  regions of the input space. We believe dropping bias terms can be considered as
  a geometric prior when designing neural network architecture for image classification,
  which shares the spirit of adapting convolutions as the translational invariance
  prior.
layout: inproceedings
series: Proceedings of Machine Learning Research
publisher: PMLR
issn: 2640-3498
id: geng24a
month: 0
tex_title: Scalar Invariant Networks with Zero Bias
firstpage: 145
lastpage: 163
page: 145-163
order: 145
cycles: false
bibtex_author: Geng, Chuqin and Xu, Xiaojie and Ye, Haolin and Si, Xujie
author:
- given: Chuqin
  family: Geng
- given: Xiaojie
  family: Xu
- given: Haolin
  family: Ye
- given: Xujie
  family: Si
date: 2024-08-02
address:
container-title: Proceedings of the 2nd NeurIPS Workshop on Symmetry and Geometry
  in Neural Representations
volume: '228'
genre: inproceedings
issued:
  date-parts:
  - 2024
  - 8
  - 2
pdf: https://raw.githubusercontent.com/mlresearch/v228/main/assets/geng24a/geng24a.pdf
extras: []
# Format based on Martin Fenner's citeproc: https://blog.front-matter.io/posts/citeproc-yaml-for-bibliographies/
---
